{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import datetime\n",
    "\n",
    "#Change the S3_Bucket created for cloudformation output bucket name S3BucketName.\n",
    "AWS_S3_BUCKET = \"siemensstack-siemensindustryedgedemo614cc6a9-lowjzf5q2kuy\"\n",
    "\n",
    "data = None\n",
    "\n",
    "s3_client = boto3.resource(\n",
    "    \"s3\"\n",
    ")\n",
    "\n",
    "date = datetime.datetime.now().date();\n",
    "year = date.strftime(\"%Y\")\n",
    "\n",
    "s3Bucket = s3_client.Bucket(AWS_S3_BUCKET)\n",
    "for my_bucket_object in s3Bucket.objects.all():\n",
    "    if my_bucket_object.key.startswith(year):\n",
    "        response = boto3.client('s3').get_object(Bucket=AWS_S3_BUCKET, Key=my_bucket_object.key)\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "        if status == 200:\n",
    "            csvdata = pd.read_csv(response.get(\"Body\"))\n",
    "            if data is None:\n",
    "                data = csvdata\n",
    "            else:\n",
    "                data = pd.concat([data, csvdata], axis=0)\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 get_object response. Status - {status}\")\n",
    "data['playload.vals.ts'] = pd.to_datetime(data['playload.vals.ts'], format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "meta = json.load(open('meta.json'))\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPointDefinitions = meta['connections'][0]['dataPoints'][0]['dataPointDefinitions']\n",
    "dataPointDefinitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPointDefinitions_df = pd.DataFrame(dataPointDefinitions)\n",
    "dataPointDefinitions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPointDefinitions_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1Min'  # '1Min' or '1H'\n",
    "id_feature = 'clientID'\n",
    "label_feature = 'playload.vals.val'  # 'pdu3_current1' or 'pdu3_current2'\n",
    "time_feature = 'playload.vals.ts'\n",
    "sparse_features = ['clientID', 'topic', 'protocol', 'playload.seq', 'playload.vals.id', 'playload.vals.qc']\n",
    "dynamic_dense_features = []\n",
    "start_time = data[time_feature].min()\n",
    "end_time = data[time_feature].max()\n",
    "print('start_time:', start_time)\n",
    "print('end_time:', end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sparse_feature in sparse_features:\n",
    "    print(sparse_feature+':', len(data[sparse_feature].unique()), data[sparse_feature].unique()[:5], '... na:', sum(data[sparse_feature].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeseries(df, dense_feature):\n",
    "    df_group = df.groupby([id_feature])\n",
    "    dense_df = pd.DataFrame({time_feature: [start_time, end_time]})\n",
    "    dense_df.set_index(time_feature, inplace=True)\n",
    "    # print(dense_df)\n",
    "    dense_df = dense_df.resample(freq).asfreq()\n",
    "    for name, group in df_group:\n",
    "        tmp_df = pd.DataFrame({name: group[dense_feature], time_feature:group[time_feature]})\n",
    "        tmp_df.set_index(time_feature, inplace=True)\n",
    "        if dense_feature == id_feature:\n",
    "            tmp_df = tmp_df.resample(freq).mean()\n",
    "        else:\n",
    "            tmp_df = tmp_df.resample(freq).mean()\n",
    "#         print(tmp_df)\n",
    "        dense_df = dense_df.join(tmp_df)\n",
    "    num_timeseries = len(df[id_feature].unique())\n",
    "    if dense_feature == id_feature:\n",
    "        dense_df = dense_df.resample(freq).mean()\n",
    "    else:\n",
    "        dense_df = dense_df.resample(freq).mean()    \n",
    "        \n",
    "    # TODO fill NaN\n",
    "    dense_df = dense_df.replace([np.inf, -np.inf], np.nan)\n",
    "    dense_df.fillna(method='ffill', inplace=True)\n",
    "    dense_df.fillna(method='bfill', inplace=True)\n",
    "    dense_df.fillna(0, inplace=True)\n",
    "        \n",
    "    timeseries = []\n",
    "    for i in range(num_timeseries):\n",
    "        timeseries.append(dense_df.iloc[:,i])\n",
    "#     print(timeseries)\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_timeseries(timeseries, dense_feature):\n",
    "    row_num = math.ceil(len(timeseries)/2)\n",
    "    fig, axs = plt.subplots(min(row_num, 2), 2, figsize=(20, 20), sharex=True)\n",
    "    axx = axs.ravel()\n",
    "    for i in range(0, min(len(timeseries), 4)):\n",
    "        timeseries[i].plot(ax=axx[i])\n",
    "        axx[i].set_xlabel(\"date\")\n",
    "        axx[i].set_ylabel(dense_feature)\n",
    "        axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_timeseries(timeseries, filename):\n",
    "    with open(filename, 'wb') as fp:\n",
    "        data = [\n",
    "            {\n",
    "                \"start\": str(timeseries[i].index[0]),\n",
    "                \"target\": timeseries[i].tolist()\n",
    "            }\n",
    "            for i in range(len(timeseries))\n",
    "        ]\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).replace('NaN', '\"NaN\"').encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['playload.vals.id']==101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['playload.vals.id']==105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('playload.vals.id:', '101', sum(data[data['playload.vals.id']==101]['playload.vals.val'].isna()))\n",
    "data_timeseries = get_timeseries(data[data['playload.vals.id']==101], label_feature)\n",
    "visualize_timeseries(data_timeseries, label_feature)\n",
    "save_timeseries(data_timeseries, 'output/'+label_feature+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('playload.vals.id:', '105', sum(data[data['playload.vals.id']==105]['playload.vals.val'].isna()))\n",
    "data_timeseries = get_timeseries(data[data['playload.vals.id']==105], label_feature)\n",
    "visualize_timeseries(data_timeseries, label_feature)\n",
    "save_timeseries(data_timeseries, 'output/'+label_feature+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for vals_id in list(data['playload.vals.id'].unique()):\n",
    "    print('playload.vals.id:', vals_id, sum(data[data['playload.vals.id']==vals_id]['playload.vals.val'].isna()))\n",
    "    data_timeseries = get_timeseries(data[data['playload.vals.id']==vals_id], label_feature)\n",
    "    visualize_timeseries(data_timeseries, label_feature)\n",
    "    save_timeseries(data_timeseries, 'output/'+label_feature+'_'+str(vals_id)+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022-04-13 to 2022-04-19\n",
    "\n",
    "DATETIME_START_OF_TRAIN = \"2022-05-13 00:00:00\"\n",
    "DATETIME_END_OF_TRAIN = \"2022-05-13 24:00:00\"\n",
    "DATETIME_START_OF_TEST = DATETIME_END_OF_TRAIN\n",
    "DATETIME_END_OF_TEST = \"2022-05-13 23:59:59\"\n",
    "DATETIME_START_OF_PREDICT = DATETIME_END_OF_TEST\n",
    "DATETIME_END_OF_PREDICT = \"2022-05-13 07:00:00\"\n",
    "\n",
    "freq = '1Min'\n",
    "prediction_length = 10\n",
    "context_length = 24*60\n",
    "\n",
    "target_vals_id = 105  # 101, 105, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(DATETIME_START_OF_TRAIN, freq=freq)\n",
    "end_training = pd.Timestamp(DATETIME_END_OF_TRAIN, freq=freq)\n",
    "start_test = pd.Timestamp(DATETIME_START_OF_TEST, freq=freq)\n",
    "end_test = pd.Timestamp(DATETIME_END_OF_TEST, freq=freq)\n",
    "start_predict = pd.Timestamp(DATETIME_START_OF_PREDICT, freq=freq)\n",
    "end_predict = pd.Timestamp(DATETIME_END_OF_PREDICT, freq=freq)\n",
    "print('start_dataset:', start_dataset)\n",
    "print('end_training:', end_training)\n",
    "print('start_test:', start_test)\n",
    "print('end_test:', end_test)\n",
    "print('start_predict:', start_predict)\n",
    "print('end_predict:', end_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "if freq == '1Min':\n",
    "    max_length = (end_predict-start_dataset).days*24*60\n",
    "elif freq == '1H':\n",
    "    max_length = (end_predict-start_dataset).days*24\n",
    "elif freq == '1D':\n",
    "    max_length = (end_predict-start_dataset).days\n",
    "elif freq == '1M':\n",
    "    year1 = start_dataset.year\n",
    "    month1 = start_dataset.month\n",
    "    year2 = end_predict.year\n",
    "    month2 = end_predict.month\n",
    "    max_length = (year2-year1)*12+(month2-month1)\n",
    "print('max_length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ids = []\n",
    "data_group = data.groupby(id_feature)\n",
    "cnt = 0\n",
    "for name, group in data_group:\n",
    "    if cnt % 1000 == 0:\n",
    "        print('cnt:', cnt)\n",
    "    cnt += 1\n",
    "    # print(name)\n",
    "    # print(group)\n",
    "    new_name = str(name)\n",
    "    # print(new_name)\n",
    "    ids.append(new_name)\n",
    "\n",
    "num_timeseries = len(ids)\n",
    "print('num_timeseries:', num_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = get_timeseries(data[data['playload.vals.id']==target_vals_id], label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_dense_timeseries = []\n",
    "for vals_id in list(data['playload.vals.id'].unique()):\n",
    "    if vals_id!=target_vals_id:\n",
    "        dense_timeseries = get_timeseries(data[data['playload.vals.id']==vals_id], label_feature)\n",
    "        dynamic_dense_timeseries.append(dense_timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(timeseries[i].index[0]),\n",
    "        \"target\": timeseries[i][start_dataset:end_training][:-1].tolist(),  # We use -1, because pandas indexing includes the upper bound \n",
    "        \"dynamic_feat\": [dense_timeseries[i][start_dataset:end_training][:-1].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "#         \"cat\": [property_cat[i]],\n",
    "        \"id\": ids[i]\n",
    "    }\n",
    "    for i in range(num_timeseries)\n",
    "]\n",
    "print(len(training_data), len(timeseries[0][start_dataset:end_training][:-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Min\n",
    "test_data = []\n",
    "for i in range(num_timeseries):\n",
    "    j = 1\n",
    "    end = end_training+datetime.timedelta(minutes=j*prediction_length)\n",
    "    while end <= end_test:\n",
    "#         print(end)\n",
    "        test_data_i = {\n",
    "            \"start\": str(timeseries[i].index[0]),\n",
    "            \"target\": timeseries[i][start_dataset:end][:-1].tolist(),\n",
    "            \"dynamic_feat\": [dense_timeseries[i][start_dataset:end][:-1].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "    #         \"cat\": [property_cat[i]],\n",
    "            \"id\": ids[i]\n",
    "        }\n",
    "        j += 1\n",
    "        end = end_training+datetime.timedelta(minutes=j*prediction_length)\n",
    "        test_data.append(test_data_i)\n",
    "        print(len(test_data), len(timeseries[0][start_dataset:end][:-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1Min\n",
    "predict_data = []\n",
    "for i in range(num_timeseries):\n",
    "    j = 1\n",
    "    end = end_test+datetime.timedelta(minutes=j*prediction_length)\n",
    "    while end <= end_predict:\n",
    "#         print(end)\n",
    "        predict_data_i = {\n",
    "            \"start\": str(timeseries[i].index[0]),\n",
    "            \"target\": timeseries[i][start_dataset:end][:-1].tolist(),\n",
    "            \"dynamic_feat\": [dense_timeseries[i][start_dataset:end][:-1].tolist() for dense_timeseries in dynamic_dense_timeseries],\n",
    "    #         \"cat\": [property_cat[i]],\n",
    "            \"id\": ids[i]\n",
    "        }\n",
    "        j += 1\n",
    "        end = end_test+datetime.timedelta(minutes=j*prediction_length)\n",
    "        predict_data.append(predict_data_i)\n",
    "        print(len(predict_data), len(timeseries[0][start_dataset:end][:-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).replace('NaN', '\"NaN\"').encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train_\"+freq+\".json\", training_data)\n",
    "write_dicts_to_file(\"test_\"+freq+\".json\", test_data)\n",
    "write_dicts_to_file(\"predict_\"+freq+\".json\", predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
